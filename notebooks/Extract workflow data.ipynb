{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87a7ddd2-9cc0-4398-933a-93d517a2e899",
   "metadata": {},
   "source": [
    "This notebook analyses the workflow files we found in the downloaded repositories and extracts information about the workflows, jobs and steps for each distinct workflow file. \n",
    "\n",
    "It requires `data/workflow_files.csv.gz` and a local directory containing all the distinct workflow files extracted from the repositories. These files are generated by the `Extract workflows` notebook.\n",
    "\n",
    "The notebook generates the following files: \n",
    "\n",
    " - workflows.csv: the file contains an entry for each of the workflow files we found. For each workflow, it indicates the name of the workflow, the list of events that trigger it (including reusable workflows) and the number of jobs.\n",
    " - jobs.csv: the file contains an entry for each of the jobs in workflow files. For each job, we indicate the workflow file, the job id and name (if any), a hash of the job, whether it corresponds to (i.e., use \"uses:\") another workflow and which one and the number of steps.\n",
    " - steps.csv: the file contains an entry for each steps we found in jobs. For each step, we report on the workflow file, the job id, step name (if any), a hash of this step, step position in the job, and the name of the action (i.e., the \"uses:\" field) if any, the number of lines in the \"run:\" field, if any, a hash of these commands (if any), the number of parameters and a hash of these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70349ce7-ce18-4a24-89ca-4025d37e306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ruamel.yaml as yaml\n",
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from hashlib import sha256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1810e42d-dd5a-495b-8c53-6fca1c6a7d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to workflow files\n",
    "WORKFLOW_DIR = Path('/data/ghactions/workflows')\n",
    "\n",
    "# Path to data folder\n",
    "DATA_DIR = Path('../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "656cd745-519b-41a3-a909-256f17cef50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = (\n",
    "    pd.read_csv('../data/workflow_files.csv.gz')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744fd0c5-2a91-43ea-b1fe-3b81832f8ace",
   "metadata": {},
   "source": [
    "Let's define a function that will extract the \"interesting parts\" of the yaml files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1d972211-0277-48bf-b793-5060575afaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_workflow(path):\n",
    "    \"\"\"\n",
    "    Given a path to a workflow file, extract parts of its content and return a dictionary mimicking the parts\n",
    "    of the y(a)ml file that are of interest (see documentation in this notebook for more details). \n",
    "    \"\"\"\n",
    "    output = dict()\n",
    "    \n",
    "    # See https://yaml.readthedocs.io/en/latest/api.html#duplicate-keys\n",
    "    # some workflows have duplicated keys (e.g. \"if\" or \"env\", I don't know why...)\n",
    "    parser = yaml.YAML()\n",
    "    parser.allow_duplicate_keys = True\n",
    "    \n",
    "    with open(path) as f: \n",
    "        workflow = parser.load(f)\n",
    "    \n",
    "    if workflow is None:\n",
    "        return output\n",
    "    \n",
    "    # Name of the workflow\n",
    "    output['name'] = workflow.get('name')\n",
    "    \n",
    "    # List of events that trigger the workflow\n",
    "    on = workflow.get('on', None)\n",
    "    if isinstance(on, str):\n",
    "        output['events'] = [on]\n",
    "    elif isinstance(on, list):\n",
    "        output['events'] = list(on)\n",
    "    elif isinstance(on, dict):\n",
    "        output['events'] = list(on.keys())\n",
    "    elif on is None:\n",
    "        output['events'] = list()\n",
    "    else:\n",
    "        assert False, f'Unsupported type {type(workflow.get(\"on\"))} for workflow.on field'\n",
    "        \n",
    "    # List of jobs\n",
    "    jobs = workflow.get('jobs')\n",
    "    if job is None:\n",
    "        raise ValueError('No job defined', workflow)\n",
    "    output['jobs'] = extract_jobs(jobs)\n",
    "        \n",
    "    return output\n",
    "    \n",
    "    \n",
    "def extract_jobs(jobs):\n",
    "    output = dict()\n",
    "    \n",
    "    for id, job in jobs.items():\n",
    "        output[id] = dict()\n",
    "        \n",
    "        output[id]['name'] = job.get('name')\n",
    "        output[id]['hash'] = sha256(str.encode(str(job))).hexdigest()\n",
    "        output[id]['uses'] = job.get('uses')\n",
    "        output[id]['steps'] = extract_steps(job.get('steps', []))\n",
    "        \n",
    "    return output\n",
    "\n",
    "\n",
    "def extract_steps(steps):\n",
    "    output = []\n",
    "    \n",
    "    for i, step in enumerate(steps):\n",
    "        item = dict()\n",
    "        \n",
    "        item['name'] = step.get('name')\n",
    "        item['hash'] = sha256(str.encode(str(step))).hexdigest()\n",
    "        item['position'] = i + 1\n",
    "        item['uses'] = step.get('uses')\n",
    "        _run = step.get('run', None)\n",
    "        if _run is not None: \n",
    "            _run = str(_run)\n",
    "            item['run'] = len(_run.split('\\n'))\n",
    "            item['run_hash'] = sha256(str.encode(_run)).hexdigest()\n",
    "            \n",
    "        # Action parameters\n",
    "        if step.get('with') is not None: \n",
    "            item['parameters'] = len(step['with'])\n",
    "            item['parameters_hash'] = sha256(str.encode(str(step['with']))).hexdigest()\n",
    "        \n",
    "        output.append(item)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c4efb9-00e4-4b44-9d6c-1422ebd8f0e9",
   "metadata": {},
   "source": [
    "Let's define a thin wrapper to handle outputs and errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b9de0b35-f263-4936-a3c4-a26b0138d270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def job(filepath):\n",
    "    path = WORKFLOW_DIR / (filepath + '.yaml')\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        return extract_workflow(path)\n",
    "    except Exception as e:\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e26dd95a-188c-47c0-86f0-26799a78fad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "done = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0aff8442-20b3-4756-8c70-3a9217ff249f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 229024/229024 [04:35<00:00, 832.21it/s]\n"
     ]
    }
   ],
   "source": [
    "inputs = [x for x in df_input.workflow.drop_duplicates().to_list() if x not in done]\n",
    "\n",
    "with multiprocessing.Pool() as pool:\n",
    "    jobs = pool.imap(job, inputs)\n",
    "    for filepath, r in tqdm(zip(inputs, jobs), total=len(inputs)):\n",
    "        output.append((filepath, r))\n",
    "        done.append(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dc8bef-61e8-4e6d-9a1c-74cfc9a25a32",
   "metadata": {},
   "source": [
    "Now we can export these results as csv files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "57a1a15f-37f0-4879-9262-b6a9bdf0119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store data (they will be converted to DataFrames afterward)\n",
    "m_workflows = []\n",
    "m_jobs = []\n",
    "m_steps = []\n",
    "\n",
    "for filepath, workflow in output: \n",
    "    if isinstance(workflow, Exception):\n",
    "        # print('-', filepath, workflow)\n",
    "        continue\n",
    "\n",
    "    m_workflows.append((\n",
    "        filepath,\n",
    "        workflow.get('name'),\n",
    "        ', '.join(workflow.get('events', [])),\n",
    "        len(workflow.get('jobs', [])),\n",
    "    ))\n",
    "\n",
    "    for job_id, job in workflow.get('jobs', dict()).items():\n",
    "        m_jobs.append((\n",
    "            filepath, \n",
    "            job_id,\n",
    "            job['hash'],\n",
    "            job.get('name'),\n",
    "            job.get('uses'),\n",
    "            len(job.get('steps', [])),\n",
    "        ))\n",
    "\n",
    "        for step in job.get('steps', []):\n",
    "            m_steps.append((\n",
    "                filepath, \n",
    "                job_id,\n",
    "                step['hash'],\n",
    "                step.get('name'),\n",
    "                step['position'],\n",
    "                step.get('uses'),\n",
    "                step.get('run', 0),\n",
    "                step.get('run_hash'),\n",
    "                step.get('parameters', 0),\n",
    "                step.get('parameters_hash'),\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "901551a7-f3a5-4a07-939b-b7f623187b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228519, 432659, 2579227)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m_workflows), len(m_jobs), len(m_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0d5629e6-67a7-45f2-8d0d-3d684d903d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_workflows = (\n",
    "    pd.DataFrame(m_workflows, columns=['workflow', 'name', 'events', 'jobs'])\n",
    "    .set_index(['workflow'])\n",
    ")\n",
    "df_jobs = (\n",
    "    pd.DataFrame(m_jobs, columns=['workflow', 'id', 'hash', 'name', 'uses', 'steps'])\n",
    "    .set_index(['workflow', 'id'])\n",
    ")\n",
    "df_steps = (\n",
    "    pd.DataFrame(m_steps, columns=['workflow', 'job', 'hash', 'name', 'pos', 'uses', 'run', 'run_hash', 'parameters', 'parameters_hash'])\n",
    "    .set_index(['workflow', 'job', 'pos'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f0684765-037e-4596-a191-b03d8f9518c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_workflows.to_csv(DATA_DIR / 'workflows.csv.gz', compression='gzip')\n",
    "df_jobs.to_csv(DATA_DIR / 'jobs.csv.gz', compression='gzip')\n",
    "df_steps.to_csv(DATA_DIR / 'steps.csv.gz', compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
