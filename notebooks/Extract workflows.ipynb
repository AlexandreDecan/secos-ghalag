{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d75a0547-85e0-4432-8d76-63a1eb39011b",
   "metadata": {},
   "source": [
    "This notebook iterates on the repositories that are locally cloned, materializes monthly snapshots and extracts all the GHA workflows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96a652ef-b51f-4318-b89f-381eb5a3f7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "import datetime\n",
    "import shutil\n",
    "\n",
    "# We use ThreadPoolExecutor since this notebook is io-bounded, and multiprocessing fails\n",
    "# because we call subcommands that override STDOUT (see Python documentation for more info).\n",
    "# from multiprocessing import Pool\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from hashlib import sha256\n",
    "\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "825435f1-36d5-4ec0-a170-08898fa1baa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to repositories\n",
    "REPO_DIR = Path('/data/ghactions/repositories')\n",
    "\n",
    "# Path to workflow folder\n",
    "WORKFLOW_DIR = Path('/data/ghactions/workflows')\n",
    "\n",
    "# Snapshot dates\n",
    "DATES = pd.date_range(start='2019-11', end='2022-09', freq='MS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fda6e840-4b2e-4fed-a195-67355303eb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial list of repositories. We will scan the REPO_DIR folder to see which ones \n",
    "# were effectively extracted and have a non-empty .github/workflows/ folder\n",
    "\n",
    "FIELDS = {\n",
    "    'name': 'repository',\n",
    "    'defaultBranch': 'branch',\n",
    "    'mainLanguage': 'language',\n",
    "    'createdAt': 'created',\n",
    "    'lastCommit': 'updated',\n",
    "    'lastCommitSHA': 'commit',\n",
    "    'stargazers': 'stars',\n",
    "    'watchers': 'watchers',\n",
    "    'forks': 'forks',\n",
    "    'size': 'size',\n",
    "    'branches': 'branches',\n",
    "    'commits': 'commits',\n",
    "    'contributors': 'contributors',\n",
    "    'totalIssues': 'issues',\n",
    "    'totalPullRequests': 'prs',\n",
    "}\n",
    "\n",
    "df_input = (\n",
    "    pd.read_csv('../data-raw/repositories.csv')\n",
    "    [FIELDS.keys()]\n",
    "    .rename(columns=FIELDS)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6933d40b-3573-422a-a83a-a635f2935dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def commit_for_date(path, branch, date):\n",
    "    \"\"\"\n",
    "    Return the sha and date of the latest commit (from given branch) before given date. \n",
    "    \"\"\"\n",
    "    cmd = partial(subprocess.run, cwd=path, capture_output=True, timeout=None)\n",
    "    date = date.strftime('%Y-%m-%d')\n",
    "    out = cmd(['git', 'rev-list', '-n 1', '--before', date, branch, '--format=%H %cs'])\n",
    "    \n",
    "    out = out.stdout.decode().strip()\n",
    "    if len(out) == 0:\n",
    "        return None, None\n",
    "    else:\n",
    "        commit, date = out.split('\\n')[1].split(' ')\n",
    "        return commit, datetime.date.fromisoformat(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b04a0d02-ee6b-47d2-82c3-7c959995e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def workflow_filenames(path):\n",
    "    \"\"\"\n",
    "    Return a list of (relative) filenames corresponding to the workflows\n",
    "    available in given repository path. \n",
    "    \"\"\"\n",
    "    filepaths = []\n",
    "    workflow_path = path / '.github/workflows'\n",
    "    \n",
    "    if workflow_path.exists():\n",
    "        for file in workflow_path.iterdir():\n",
    "            if file.suffix in ['.yaml', '.yml']:\n",
    "                filepaths.append(file.name)\n",
    "    return filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff5da782-12f1-4b5d-b0c1-a745cd64648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sha_for_file(path):\n",
    "    \"\"\"\n",
    "    Return a sha-256 hash for given file. \n",
    "    \"\"\"\n",
    "    file_hash = sha256()\n",
    "    with open(path, 'rb') as f: \n",
    "        while True: \n",
    "            chunk = f.read(file_hash.block_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            file_hash.update(chunk)\n",
    "            \n",
    "    return file_hash.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18f57e65-9126-48a9-bd19-422b7b0115b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkout_workflows(path, commit):\n",
    "    \"\"\"\n",
    "    Checkout the .github/workflows path of given repository for given commit.\n",
    "    \"\"\"\n",
    "    cmd = partial(subprocess.run, cwd=path, capture_output=True, timeout=None)\n",
    "    cmd(['git', 'checkout', '-f', commit, '--no-overlay', '.github'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ed5569d-0b85-4afe-974d-813f5a9e2c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_workflows(path, output_dir):\n",
    "    \"\"\"\n",
    "    Copy the workflows available in given repository to the output directory.\n",
    "    Each workflow will receive a new filename corresponding to its hash. \n",
    "    Return a list of (filename, hash). \n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    filenames = workflow_filenames(path)\n",
    "\n",
    "    for filename in filenames: \n",
    "        filepath = path / '.github/workflows' / filename\n",
    "        sha = sha_for_file(filepath)\n",
    "        \n",
    "        output_file = (output_dir / (sha+'.yaml'))\n",
    "        if not output_file.exists():\n",
    "            shutil.copyfile(filepath, output_file)\n",
    "            #output_file.write_text(filepath.read_text())\n",
    "            \n",
    "        results.append((filename, sha))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f7afe0-c5e9-4f12-bc6e-0a54afb6311e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e81dd5-63c1-4f4c-8f14-6a8f436c11ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "feb61a23-af79-4728-b990-4307cc818c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def job(repo, branch, dates):\n",
    "    path = REPO_DIR / repo.replace('/', '---')\n",
    "    \n",
    "    # Check that repository exists\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    \n",
    "    # Check repository has workflows in its latest commit\n",
    "    checkout_workflows(path, branch)\n",
    "    if len(workflow_filenames(path)) == 0:\n",
    "        return None\n",
    "    \n",
    "    output = dict()\n",
    "    for date in reversed(dates): \n",
    "        commit, commit_date = commit_for_date(path, branch, date)\n",
    "        # Is there a commit for given date?\n",
    "        if commit is None: \n",
    "            break  # Dates are in reversed order!\n",
    "        \n",
    "        # Checkout repository\n",
    "        checkout_workflows(path, commit)\n",
    "        \n",
    "        # Copy workflows\n",
    "        workflows = copy_workflows(path, WORKFLOW_DIR)\n",
    "\n",
    "        output[date] = {\n",
    "            'commit': commit,\n",
    "            'commit_date': commit_date, \n",
    "            'workflows': workflows\n",
    "        }\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "228ff3c8-e958-4c5f-b629-584822b5b6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "done = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5314566d-5f6a-4516-ad95-9ea9a2b1218a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/64008 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "inputs = [(x.repository, x.branch, DATES) for x in df_input.itertuples() if x.repository not in done]\n",
    "\n",
    "with ThreadPoolExecutor() as pool:\n",
    "    jobs = pool.map(job, *zip(*inputs))\n",
    "    \n",
    "    for (repo, _, _), workflows in tqdm(zip(inputs, jobs), smoothing=0, miniters=1, total=len(inputs)):\n",
    "        if workflows is not None: \n",
    "            output.append((repo, workflows))\n",
    "        done.append(repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9843cf3-0a97-4b97-8486-6c5208d5ffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for repo, snapshots in output:\n",
    "    for date, snapshot in snapshots.items():\n",
    "        for filepath, sha in snapshot['workflows']:\n",
    "            data.append((\n",
    "                repo, \n",
    "                date, \n",
    "                snapshot['commit'],\n",
    "                snapshot['commit_date'],\n",
    "                filepath, \n",
    "                sha\n",
    "            ))\n",
    "            \n",
    "df_workflows = pd.DataFrame(data=data, columns=['repository', 'date', 'commit', 'commit_date', 'filename', 'workflow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b620b8b1-ca6f-4dad-8cbd-e8691649748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_workflows.to_csv('../data/workflow_files.csv.gz', compression='gzip', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
